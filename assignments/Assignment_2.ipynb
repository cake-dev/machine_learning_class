{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Machine Learning Basics\n",
    "\n",
    "## *Jake Bova*\n",
    "Netid:  *jb240893*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for all assignments can be found [here](https://github.com/kylebradbury/ids705/blob/master/assignments/_Assignment%20Instructions.ipynb), which is also linked to from the [course syllabus](https://kylebradbury.github.io/ids705/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "This assignment will provide structured practice to help enable you to...\n",
    "- Implement a k-nearest neighbors machine learning algorithm from scratch in a style similar to that of popular machine learning tools like `scikit-learn`\n",
    "- Apply basic regression and classification supervised learning techniques to data and evaluate the performance of those methods\n",
    "- Understand the bias-variance tradeoff and the impact of model flexibility algorithm performance and model selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAC USERS TAKE NOTE:\n",
    "# For clearer plots in Jupyter notebooks on macs, run the following line of code:\n",
    "# %config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "**[5 points]**\n",
    "For each part (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.\n",
    "\n",
    "1. The sample size $n$ is extremely large, and the number of predictors $p$ is small.\n",
    "2. The number of predictors $p$ is extremely large, and the number of observations $n$ is small.\n",
    "3. The relationship between the predictors and response is highly non-linear.\n",
    "4. The variance of the error terms, i.e. $\\sigma^2 = Var(\\epsilon)$, is extremely high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Better. With a large sample size, the flexible method will be able to fit the data more closely, and the variance will be lower.\n",
    "\n",
    "2. Worse. With a small sample size, the flexible method will overfit the data, and the variance will be higher.\n",
    "\n",
    "3. Better. A flexible method will be able to fit the non-linear relationship more closely, and the variance will be lower.\n",
    "\n",
    "4. Worse. A flexible method will overfit the data, and the variance will be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "**[5 points]** For each of the following, (i) explain if each scenario is a classification or regression problem, (ii) indicate whether we are most interested in inference or prediction for that problem, and (iii) provide the sample size $n$ and number of predictors $p$ indicated for each scenario.\n",
    "\n",
    "**(a)** We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.\n",
    "\n",
    "**(b)** We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n",
    "\n",
    "**(c)** We are interesting in predicting the % change in the US dollar in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the dollar, the % change in the US market, the % change in the British market, and the % change in the German market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) (i) Regression (ii) Inference (iii) n = 500, p = 3\n",
    "\n",
    "b) (i) Classification (ii) Prediction (iii) n = 20, p = 13\n",
    "\n",
    "c) (i) Regression (ii) Prediction (iii) n = 52, p = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "**[10 points] Classification II**. The table below provides a training dataset containing six observations ($n=6$), three predictors ($p=3$), and one qualitative response variable.\n",
    "\n",
    "*Table 1. Dataset with $n=6$ observations in $p=3$ dimensions with a categorical response, $y$*\n",
    "\n",
    "| Obs. | $x_1$ | $x_2$ | $x_3$ | $y$   |\n",
    "|------|-------|-------|-------|-------|\n",
    "| **1**| 0     | 3     | 0     | Red   |\n",
    "| **2**| 2     | 0     | 0     | Red   |\n",
    "| **3**| 0     | 1     | 3     | Red   |\n",
    "| **4**| 0     | 1     | 2     | Blue  |\n",
    "| **5**| -1    | 0     | 1     | Blue  |\n",
    "| **6**| 1     | 1     | 1     | Red   |\n",
    "\n",
    "We want to use this dataset to make a prediction for $y$ when $x_1=x_2=x_3=0$ using $K$-nearest neighbors. You are given some code below to get you started.\n",
    "\n",
    "**(a)** Compute the Euclidean distance between each observation and the test point, $x_1=x_2=x_3=0$. Present your answer in a table similar in style to Table 1 with observations 1-6 as the row headers.\n",
    "\n",
    "**(b)** What is our prediction with $K=1$? Why?\n",
    "\n",
    "**(c)** What is our prediction with $K=3$? Why?\n",
    "\n",
    "**(d)** If the Bayes decision boundary (the optimal decision boundary) in this problem is highly nonlinear, then would we expect the *best* value of $K$ to be large or small? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[ 0, 3, 0],\n",
    "              [ 2, 0, 0],\n",
    "              [ 0, 1, 3],\n",
    "              [ 0, 1, 2],\n",
    "              [-1, 0, 1],\n",
    "              [ 1, 1, 1]])\n",
    "y = np.array(['r','r','r','b','b','r'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observation</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.162278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.236068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1.732051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Observation  Distance\n",
       "0            1  3.000000\n",
       "1            2  2.000000\n",
       "2            3  3.162278\n",
       "3            4  2.236068\n",
       "4            5  1.414214\n",
       "5            6  1.732051"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "distances = []\n",
    "for i in range(len(X)):\n",
    "    distances.append(distance.euclidean(X[i], [0,0,0]))\n",
    "\n",
    "df_results = pd.DataFrame({'Observation': [1,2,3,4,5,6], 'Distance': distances})\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Our prediction with $K=1$ is blue, because the nearest neighbor is observation 5, which is blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Our prediction with $K=3$ is red, because the three nearest neighbors are observations 2, 5, and 6, which give 2r and 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** We would expect the best value of $K$ to be small, because a small value of $K$ will be more flexible and will be able to fit the non-linear boundary more closely.  If we used a high $K$ value, the boundary would be more smooth / linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "**[20 points] Classification I: Creating a classification algorithm**.\n",
    "\n",
    "**(a)** Build a working version of a binary kNN classifier using the skeleton code below.\n",
    "\n",
    "**(b)** Load the datasets to be evaluated here. Each includes training features ($\\mathbf{X}$), and test features ($\\mathbf{y}$) for both a low dimensional ($p = 2$ features/predictors) and a high dimensional ($p = 100$ features/predictors). For each of these datasets there are $n=100$ observations of each. They can be found in the `data` subfolder in the `assignments` folder on github. Each file is labeled similar to `A2_X_train_low.csv`, which lets you know whether the dataset is of features, $X$, targets, $y$; training or testing; and low or high dimensions.\n",
    "\n",
    "**(c)** Train your classifier on first the low dimensional dataset and then the high dimensional dataset with $k=5$. Evaluate the classification performance on the corresponding test data for each. Calculate the time it takes to make the predictions in each case and the overall accuracy of each set of test data predictions.\n",
    "\n",
    "**(d)** Compare your implementation's accuracy and computation time to the scikit learn [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) class. How do the results and speed compare?\n",
    "\n",
    "**(e)** Some supervised learning algorithms are more computationally intensive during training than testing. What are the drawbacks of the prediction process being slow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Knn:\n",
    "# k-Nearest Neighbor class object for classification training and testing\n",
    "    def __init__(self):\n",
    "        self.x_train = None\n",
    "        self.y_train = None\n",
    "        self.k = None\n",
    "        \n",
    "    def fit(self, x, y, k):\n",
    "        # Save the training data to properties of this class\n",
    "        self.x_train = x # features\n",
    "        self.y_train = y # targets / labels\n",
    "        self.k = k # number of neighbors to use\n",
    "        \n",
    "    def predict(self, x):\n",
    "        y_hat = [] # Variable to store the estimated class label for each observation in x\n",
    "        # Calculate the distance from each vector in x to the training data\n",
    "        for x_test in x:\n",
    "            distances = [] # each x_test will have its own list of distances\n",
    "            for x_train in self.x_train:\n",
    "                distances.append(distance.euclidean(x_test, x_train))\n",
    "            \n",
    "            # identify the k nearest neighbors\n",
    "            k_neighbor_indices = np.argsort(distances)[:self.k] # argsort returns the indices that would sort an array, slice to get the k smallest\n",
    "            # print('k_neighbor_indices: ', k_neighbor_indices)\n",
    "\n",
    "            # identify the class of each neighbor\n",
    "            k_neighbor_classes = [self.y_train[i] for i in k_neighbor_indices] # use the indices to get the classes from the training data\n",
    "            # print('k_neighbor_classes: ', k_neighbor_classes)\n",
    "\n",
    "            # identify the most common class among the neighbors\n",
    "            y_hat.append(max(set(k_neighbor_classes), key=k_neighbor_classes.count)) # count the number of times each class appears, return the most common\n",
    "\n",
    "        # return the estimated targets\n",
    "        # print('y_hat: ', y_hat)\n",
    "        return y_hat\n",
    "\n",
    "# Metric of overall classification accuracy\n",
    "#  (a more general function, sklearn.metrics.accuracy_score, is also available)\n",
    "def accuracy(y,y_hat):\n",
    "    nvalues = len(y)\n",
    "    accuracy = sum(y == y_hat) / nvalues\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.512</td>\n",
       "      <td>1.123616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  accuracy      time\n",
       "0  5     0.512  1.123616"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the performance of your kNN classifier on a low- and a high-dimensional dataset \n",
    "#   and time the predictions of each\n",
    "\n",
    "# low-dimensional dataset\n",
    "X = np.array([[ 0, 3, 0],\n",
    "                [ 2, 0, 0],\n",
    "                [ 0, 1, 3],\n",
    "                [ 0, 1, 2],\n",
    "                [-1, 0, 1],\n",
    "                [ 1, 1, 1]])\n",
    "y = np.array(['r','r','r','b','b','r'])\n",
    "\n",
    "# high-dimensional dataset\n",
    "Xhigh = np.random.rand(1000, 100)\n",
    "yhigh = np.random.randint(0, 2, 1000)\n",
    "\n",
    "# split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train_high, X_test_high, y_train_high, y_test_high = train_test_split(Xhigh, yhigh)\n",
    "\n",
    "# function to time predictions of a model on a dataset\n",
    "import time\n",
    "import pandas as pd\n",
    "def time_custom_knn(X_train, X_test, y_train, y_test, k):\n",
    "    knn = Knn()\n",
    "    knn.fit(X_train, y_train, k)\n",
    "    start_time = time.time()\n",
    "    y_pred = knn.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    accuracy_score = accuracy(y_test, y_pred)\n",
    "    output = pd.DataFrame({'k': [k], 'accuracy': [accuracy_score], 'time': [elapsed_time]})\n",
    "    return output\n",
    "\n",
    "# time the predictions of the low-dimensional dataset\n",
    "# time_custom_knn(X_train, X_test, y_train, y_test, 5)\n",
    "\n",
    "# time the predictions of the high-dimensional dataset\n",
    "time_custom_knn(X_train_high, X_test_high, y_train_high, y_test_high, 5)\n",
    "\n",
    "# # # predict the point 0,0,0 (to compare with Q3)\n",
    "# ypred = knn.predict([[0,0,0]])\n",
    "# print('ypred: ', ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.084128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  accuracy      time\n",
       "0  5     0.512  0.084128"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def time_scikit_knn(X_train, y_train, X_test, y_test, k):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    startime = time.time()\n",
    "    y_pred = knn.predict(X_test)\n",
    "    endtime = time.time()\n",
    "    runtime = endtime - startime\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    output = pd.DataFrame({'k': [k], 'accuracy': [accuracy], 'time': [runtime]})\n",
    "    return output\n",
    "\n",
    "# time predictions of the low-dimensional dataset\n",
    "# time_scikit_knn(X_train, y_train, X_test, y_test, 5)\n",
    "\n",
    "# time predictions of the high-dimensional dataset\n",
    "time_scikit_knn(X_train_high, y_train_high, X_test_high, y_test_high, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "**[20 points] Bias-variance tradeoff I: Understanding the tradeoff**. This exercise will illustrate the impact of the bias-variance tradeoff on classifier performance by looking at classifier decision boundaries.\n",
    "\n",
    "**(a)** Create a synthetic dataset (with both features and targets). Use the [`make_moons`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html#sklearn.datasets.make_moons) module with the parameter `noise=0.35` to generate 1000 random samples.\n",
    "\n",
    "**(b)** Scatterplot your random samples with each class in a different color\n",
    "\n",
    "**(c)** Create 3 different data subsets by selecting 100 of the 1000 data points at random three times. For each of these 100-sample datasets, fit three k-Nearest Neighbor classifiers with: $k = \\{1, 25, 50\\}$. This will result in 9 combinations (3 datasets, with 3 trained classifiers).\n",
    "\n",
    "**(d)** For each combination of dataset trained classifier, in a 3-by-3 grid, plot the decision boundary (similar in style to Figure 2.15 from *Introduction to Statistical Learning*). Each column should represent a different value of $k$ and each row should represent a different dataset.\n",
    "\n",
    "**(e)** What do you notice about the difference between the rows and the columns. Which decision boundaries appear to best separate the two classes of data? Which decision boundaries vary the most as the data change?\n",
    "\n",
    "**(f)** Explain the bias-variance tradeoff using the example of the plots you made in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "**[20 points] Bias-variance trade-off II: Quantifying the tradeoff**. This exercise will explore the impact of the bias-variance tradeoff on classifier performance by looking at classifier decision boundaries.\n",
    "\n",
    "Here, the value of $k$ determines how flexible our model is.\n",
    "\n",
    "**(a)** Using the function created earlier to generate random samples (using the `make_moons` function), create a new set of 1000 random samples, and call this dataset your test set and the previously created dataset your training set.\n",
    "\n",
    "**(b)** Train a kNN classifier on your training set for $k = 1,2,...500$. Apply each of these trained classifiers to both your training dataset and your test dataset and plot the classification error (fraction of mislabeled datapoints).\n",
    "\n",
    "**(c)** What trend do you see in the results?\n",
    "\n",
    "**(d)** What values of $k$ represent high bias and which represent high variance?\n",
    "\n",
    "**(e)** What is the optimal value of $k$ and why?\n",
    "\n",
    "**(f)** In kNN classifiers, the value of k controls the flexibility of the model - what controls the flexibility of other models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\n",
    "**[20 points] Linear regression and nonlinear transformations**. You're given training and testing data contained in files \"A2_Q7_train.csv\" and \"A2_Q7_test.csv\" in the \"data\" folder for this assignment. Your goal is to develop a regression algorithm from the training data that performs well on the test data.\n",
    "\n",
    "*Hint: Use the scikit learn [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) module.*\n",
    "\n",
    "**(a)** Create a scatter plot of your training data.\n",
    "\n",
    "**(b)** Estimate a linear regression model ($y = a_0 + a_1 x$) for the training data and calculate both the $R^2$ value and mean square error for the fit of that model for the training data. Also provide the equation representing the estimated model (e.g. $y = a_0 + a_1 x$, but with the estimated coefficients inserted.\n",
    "\n",
    "**(c)** If features can be nonlinearly transformed, a linear model may incorporate those non-linear feature transformation relationships in the training process. From looking at the scatter plot of the training data, choose a transformation of the predictor variable, $x$ that may make sense for these data. This will be a multiple regression model of the form $y = a_0 + a_1 x_1 + a_2 x_2 + \\ldots + a_n x_n$. Here $x_i$ could be any transformations of x - perhaps it's $\\frac{1}{x}$, $log(x)$, $sin(x)$, $x^k$ (where $k$ is any power of your choosing). Provide the estimated equation for this multiple regression model (e.g. if you chose your predictors to be $x_1 = x$ and $x_2 = log(x)$, your model would be of the form $y = a_0 + a_1 x + a_2 log(x)$. Also provide the $R^2$ and mean square error of the fit for the training data.\n",
    "\n",
    "**(d)** Using both of the models you created here in (b) and (c), plot the original data (as a scatter plot) and the two curves representing your models (each as a separate line).\n",
    "\n",
    "**(e)** Using the models above, apply them to the test data and estimate the $R^2$ and mean square error of the test dataset.\n",
    "\n",
    "**(f)** Which models perform better on the training data, and which on the test data? Why?\n",
    "\n",
    "**(g)** Imagine that the test data were significantly different from the training dataset. How might this affect the predictive capability of your model? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
