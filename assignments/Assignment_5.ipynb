{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 - Kaggle Competition and Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *YOUR FULL NAME HERE*\n",
    "Netid: Your netid here\n",
    "\n",
    "*Names of students you worked with on this assignment*: LIST HERE IF APPLICABLE (delete if not)\n",
    "\n",
    "Note: this assignment falls under collaboration Mode 2: Individual Assignment â€“ Collaboration Permitted. Please refer to the syllabus for additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for all assignments can be found [here](https://github.com/kylebradbury/ids705/blob/master/assignments/_Assignment%20Instructions.ipynb), and is also linked to from the [course syllabus](https://kylebradbury.github.io/ids705/index.html).\n",
    "\n",
    "Total points in the assignment add up to 90; an additional 10 points are allocated to presentation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "Through completing this assignment you will be able to...\n",
    "\n",
    "2. Apply clustering techniques to a variety of datasets with diverse distributional properties, gaining an understanding of their strengths and weaknesses and how to tune model parameters\n",
    "3. Apply PCA and t-SNE for performing dimensionality reduction and data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "## [25 points] Clustering\n",
    "\n",
    "Clustering can be used to reveal structure between samples of data and assign group membership to similar groups of samples. This exercise will provide you with experience applying clustering algorithms and comparing these techniques on various datasets to experience the pros and cons of these approaches when the structure of the data being clustered varies. For this exercise, we'll explore clustering in two dimensions to make the results more tangible, but in practice these approaches can be applied to any number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Run K-means and choose the number of clusters**. Five datasets are provided for you below and the code to load them below. \n",
    "- Scatterplot each dataset\n",
    "- For each dataset run the k-means algorithm for values of $k$ ranging from 1 to 10 and for each plot the \"elbow curve\" where you plot dissimilarity in each case. Here, you can measure dissimilarity using the within-cluster sum-of-squares, which in sklean is know as \"inertia\" and can be accessed through the `inertia_` attribute of a fit KMeans class instance.\n",
    "- For each datasets, where is the elbow in the curve of within-cluster sum-of-squares and why? Is the elbow always clearly visible? When its not clear, you will have to use your judgement in terms of selecting a reasonable number of clusters for the data. *There are also other metrics you can use to explore to measure the quality of cluster fit (but do not have to for this assignment) including the silhouette score, the Calinski-Harabasz index, and the Davies-Bouldin, to name a few within sklearn alone. However, assessing quality of fit without \"preferred\" cluster assignments to compare against (that is, in a truly unsupervised manner) is challenging because measuring cluster fit quality is typically poorly-defined and doesn't generalize across all types of inter- and intra-cluster variation.*\n",
    "- Plot your clustered data (different color for each cluster assignment) for your best $k$-means fit determined from both the elbow curve and your judgement for each dataset and your inspection of the dataset.\n",
    "\n",
    "**(b) Apply DBSCAN**. Vary the `eps` and `min_samples` parameters to get as close as you can to having the same number of clusters as your choices with K-means. In this case, the black points are points that were not assigned to clusters.\n",
    "\n",
    "**(c) Apply Spectral Clustering**. Select the same number of clusters as selected by k-means.\n",
    "\n",
    "**(d) Comment on the strengths and weaknesses of each approach**. In particular, mention: \n",
    "- Which technique worked \"best\" and \"worst\" (as defined by matching how human intuition would cluster the data) on each dataset?\n",
    "- How much effort was required to get good clustering for each method (how much parameter tuning needed to be done)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: for these clustering plots in this question, do NOT include legends indicating cluster assignment; instead just make sure the cluster assignments are clear from the plot (e.g. different colors for each cluster)*\n",
    "\n",
    "Code is provided below for loading the datasets and for making plots with the clusters as distinct colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Load the data\n",
    "################################\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "\n",
    "# Create / load the datasets:\n",
    "n_samples = 1500\n",
    "X0, _ = make_blobs(n_samples=n_samples, centers=2, n_features=2, random_state=0)\n",
    "X1, _ = make_blobs(n_samples=n_samples, centers=5, n_features=2, random_state=0)\n",
    "\n",
    "random_state = 170\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state, cluster_std=1.3)\n",
    "transformation = [[0.6, -0.6], [-0.2, 0.8]]\n",
    "X2 = np.dot(X, transformation)\n",
    "X3, _ = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state)\n",
    "X4, _ = make_moons(n_samples=n_samples, noise=.12)\n",
    "\n",
    "X = [X0, X1, X2, X3, X4]\n",
    "# The datasets are X[i], where i ranges from 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Code to plot clusters\n",
    "################################\n",
    "def plot_cluster(ax, data, cluster_assignments):\n",
    "    '''Plot two-dimensional data clusters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib axis\n",
    "        Axis to plot on\n",
    "    data : list or numpy array of size [N x 2] \n",
    "        Clustered data\n",
    "    cluster_assignments : list or numpy array [N]\n",
    "        Cluster assignments for each point in data\n",
    "\n",
    "    '''\n",
    "    clusters = np.unique(cluster_assignments)\n",
    "    n_clusters = len(clusters)\n",
    "    for ca in clusters:\n",
    "        kwargs = {}\n",
    "        if ca == -1:\n",
    "            # if samples are not assigned to a cluster (have a cluster assignment of -1, color them gray)\n",
    "            kwargs = {'color':'gray'}\n",
    "            n_clusters = n_clusters - 1\n",
    "        ax.scatter(data[cluster_assignments==ca, 0], data[cluster_assignments==ca, 1],s=5,alpha=0.5, **kwargs)\n",
    "        ax.set_xlabel('feature 1')\n",
    "        ax.set_ylabel('feature 2')\n",
    "        ax.set_title(f'No. Clusters = {n_clusters}')\n",
    "        ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "\n",
    "## [25 points] Dimensionality reduction and visualization of digits with PCA and t-SNE\n",
    "\n",
    "**(a)** Reduce the dimensionality of the data with PCA for data visualization. Load the `scikit-learn` digits dataset (code provided to do this below). Apply PCA and reduce the data (with the associated cluster labels 0-9) into a 2-dimensional space. Plot the data with labels in this two dimensional space (labels can be colors, shapes, or using the actual numbers to represent the data - definitely include a legend in your plot).\n",
    "\n",
    "**(b)** Create a plot showing the cumulative fraction of variance explained as you incorporate from $1$ through all $D$ principal components of the data (where $D$ is the dimensionality of the data). \n",
    "- What fraction of variance in the data is UNEXPLAINED by the first two principal components of the data? \n",
    "- Briefly comment on how this may impact how well-clustered the data are. \n",
    "*You can use the `explained_variance_` attribute of the PCA module in `scikit-learn` to assist with this question*\n",
    "\n",
    "**(c)** Reduce the dimensionality of the data with t-SNE for data visualization. T-distributed stochastic neighborhood embedding (t-SNE) is a nonlinear dimensionality reduction technique that is particularly adept at embedding the data into lower 2 or 3 dimensional spaces. Apply t-SNE using the `scikit-learn` implementation to the digits dataset and plot it in 2-dimensions (with associated cluster labels 0-9). You may need to adjust the parameters to get acceptable performance. You can read more about how to use t-SNE effectively [here](https://distill.pub/2016/misread-tsne/).\n",
    "\n",
    "**(d)** Briefy compare/contrast the performance of these two techniques. \n",
    "- Which seemed to cluster the data best and why?\n",
    "- Notice that t-SNE doesn't have a `fit` method, but only a `fit_transform` method. Why is this? What implications does this imply for using this method?\n",
    "*Note: Remember that you typically will not have labels available in most problems.*\n",
    "\n",
    "Code is provided for loading the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Load the data\n",
    "################################\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# load dataset\n",
    "digits = datasets.load_digits()\n",
    "n_sample = digits.target.shape[0]\n",
    "n_feature = digits.images.shape[1] * digits.images.shape[2]\n",
    "X_digits = np.zeros((n_sample, n_feature))\n",
    "for i in range(n_sample):\n",
    "    X_digits[i, :] = digits.images[i, :, :].flatten()\n",
    "y_digits = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nteract": {
   "version": "0.22.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "644px",
    "left": "1473px",
    "right": "20px",
    "top": "122px",
    "width": "367px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
